{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from pinecone import ServerlessSpec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key)\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"ia-sensor\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "   pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sem prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Para configurar a central para o cliente, siga os seguintes passos:\n",
       "\n",
       "1. Acesse o PUTTY e execute o comando sensorgw appConfig.\n",
       "2. Confirme a configuração padrão do servidor.\n",
       "3. Insira o nome do cliente no campo Client WebApp.\n",
       "4. Insira o login e senha do cliente.\n",
       "5. Verifique a conexão e o modem.\n",
       "6. Defina a operadora no campo Carrier.\n",
       "7. Sincronize as configurações com o comando sync.\n",
       "8. Pare o serviço com sensorgw stop.\n",
       "9. Configure o HAMACHI conforme necessário.\n",
       "10. Acesse o portal logmein para aceitar solicitações pendentes.\n",
       "11. Verifique se os sensores estão enviando dados para o portal do cliente.\n",
       "12. Realize testes de conexão e envio de dados.\n",
       "13. Acesse o portal do cliente para configurar as redes.\n",
       "14. Salve e aplique um Hard Reboot na central para gravar as informações.\n",
       "15. Separe o kit com a central e a fonte testada.\n",
       "16. Preencha a planilha de Clientes com os dados da central e sensores.\n",
       "\n",
       "Esses passos devem ajudar a configurar a central para o cliente de forma adequada."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload do PDFs\n",
    "file = \"files/Configuração e teste - Central SensorGW.pdf\"\n",
    "loader = PyMuPDFLoader(file)\n",
    "docs = loader.load()\n",
    "\n",
    "# Extrair texto dos documentos\n",
    "texts = []\n",
    "for doc in docs:\n",
    "    texts.append(doc.page_content)\n",
    "\n",
    "# Dividir texto em chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "split_texts = text_splitter.split_documents(docs)\n",
    "\n",
    "# Gera embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Criação do armazenamento vetorial no Pinecone\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=split_texts, \n",
    "    embedding=embeddings, \n",
    "    index_name=index_name \n",
    ")\n",
    "\n",
    "# Configuração do modelo\n",
    "llm = ChatOpenAI(   \n",
    "    model='gpt-3.5-turbo',  \n",
    "    temperature=0.0  \n",
    ")  \n",
    "\n",
    "#  Configuração de mecanismo de perguntas e respostas\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")\n",
    "\n",
    "contexto = \"\\n\".join(texts) \n",
    "\n",
    "# Consulta para buscar informações\n",
    "pergunta = \"Como faço para congifurar a central para o cliente?\"\n",
    "\n",
    "pergunta_com_contexto = f\"Contexto:\\n{contexto}\\n\\nPergunta: {pergunta}\"\n",
    "\n",
    "resposta = llm(pergunta_com_contexto)\n",
    "\n",
    "display(Markdown(resposta.content)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Com prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload do PDFs\n",
    "file = \"files/Configuração e teste - Central SensorGW.pdf\"\n",
    "loader = PyMuPDFLoader(file)\n",
    "docs = loader.load()\n",
    "\n",
    "# Extrair texto dos documentos\n",
    "texts = []\n",
    "for doc in docs:\n",
    "    texts.append(doc.page_content)\n",
    "\n",
    "# Dividir texto em chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "split_texts = text_splitter.split_documents(docs)\n",
    "\n",
    "# Gera embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Criação do armazenamento vetorial no Pinecone\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=split_texts, \n",
    "    embedding=embeddings, \n",
    "    index_name=index_name \n",
    ")\n",
    "\n",
    "# Configuração do modelo\n",
    "llm = ChatOpenAI(   \n",
    "    model='gpt-3.5-turbo',  \n",
    "    temperature=0.0  \n",
    ")  \n",
    "\n",
    "# Configuração de mecanismo de perguntas e respostas\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(  \n",
    "    llm=llm,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")\n",
    "\n",
    "contexto = \"\\n\".join(texts) \n",
    "\n",
    "# Prompt aprimorado para respostas detalhadas e passo a passo\n",
    "prompt = f\"\"\"\n",
    "Você é um assistente especializado em responder perguntas com base exclusivamente nas informações fornecidas no contexto. \n",
    "Sua resposta deve ser extremamente detalhada e organizada em um formato passo a passo. \n",
    "Se houver etapas a serem seguidas, descreva cada uma delas claramente. \n",
    "Se a resposta não puder ser encontrada no contexto, diga claramente que não sabe, sem adicionar informações externas ou especulações.\n",
    "\n",
    "Baseado no seguinte contexto:\n",
    "{contexto}\n",
    "\n",
    "Responda à pergunta:\n",
    "{pergunta}\n",
    "\n",
    "Formato esperado:\n",
    "1. Passo 1: [descrição detalhada]\n",
    "2. Passo 2: [descrição detalhada]\n",
    "...\n",
    "\n",
    "Se a resposta não estiver no contexto acima, responda: 'Não encontrei essa informação no material disponível.'\n",
    "\"\"\"\n",
    "\n",
    "# Consulta para buscar informações\n",
    "pergunta = \"Como faço para configurar ao central para o cliente em campo?\"\n",
    "\n",
    "pergunta_com_contexto = prompt\n",
    "\n",
    "resposta = llm(pergunta_com_contexto)\n",
    "\n",
    "display(Markdown(resposta.content))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
